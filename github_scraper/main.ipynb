{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.11.2-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting scrapy_user_agents\n",
      "  Downloading scrapy_user_agents-0.1.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting Twisted>=18.9.0 (from scrapy)\n",
      "  Downloading twisted-24.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cryptography>=36.0.0 (from scrapy)\n",
      "  Downloading cryptography-43.0.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cssselect>=0.9.1 (from scrapy)\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting itemloaders>=1.0.1 (from scrapy)\n",
      "  Downloading itemloaders-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting parsel>=1.5.0 (from scrapy)\n",
      "  Downloading parsel-1.9.1-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pyOpenSSL>=21.0.0 (from scrapy)\n",
      "  Downloading pyOpenSSL-24.2.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting queuelib>=1.4.2 (from scrapy)\n",
      "  Downloading queuelib-1.7.0-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting service-identity>=18.1.0 (from scrapy)\n",
      "  Downloading service_identity-24.1.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting w3lib>=1.17.0 (from scrapy)\n",
      "  Downloading w3lib-2.2.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting zope.interface>=5.1.0 (from scrapy)\n",
      "  Downloading zope.interface-7.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
      "Collecting protego>=0.1.15 (from scrapy)\n",
      "  Downloading Protego-0.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting itemadapter>=0.1.0 (from scrapy)\n",
      "  Downloading itemadapter-0.9.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/python/3.10.13/lib/python3.10/site-packages (from scrapy) (68.2.2)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.10/site-packages (from scrapy) (24.1)\n",
      "Collecting tldextract (from scrapy)\n",
      "  Downloading tldextract-5.1.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting lxml>=4.4.1 (from scrapy)\n",
      "  Downloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /home/codespace/.local/lib/python3.10/site-packages (from scrapy) (0.7.1)\n",
      "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
      "  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting user-agents (from scrapy_user_agents)\n",
      "  Downloading user_agents-2.2.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/codespace/.local/lib/python3.10/site-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\n",
      "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from service-identity>=18.1.0->scrapy) (23.2.0)\n",
      "Collecting pyasn1 (from service-identity>=18.1.0->scrapy)\n",
      "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting pyasn1-modules (from service-identity>=18.1.0->scrapy)\n",
      "  Downloading pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting automat>=0.8.0 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading Automat-22.10.0-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting constantly>=15.1 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting hyperlink>=17.1.1 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting incremental>=24.7.0 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from Twisted>=18.9.0->scrapy) (4.12.2)\n",
      "Requirement already satisfied: idna in /home/codespace/.local/lib/python3.10/site-packages (from tldextract->scrapy) (3.7)\n",
      "Requirement already satisfied: requests>=2.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from tldextract->scrapy) (2.32.3)\n",
      "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
      "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /home/codespace/.local/lib/python3.10/site-packages (from tldextract->scrapy) (3.15.4)\n",
      "Collecting ua-parser>=0.10.0 (from user-agents->scrapy_user_agents)\n",
      "  Downloading ua_parser-0.18.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six in /home/codespace/.local/lib/python3.10/site-packages (from automat>=0.8.0->Twisted>=18.9.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /home/codespace/.local/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.22)\n",
      "Requirement already satisfied: tomli in /home/codespace/.local/lib/python3.10/site-packages (from incremental>=24.7.0->Twisted>=18.9.0->scrapy) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.1.0->tldextract->scrapy) (2024.7.4)\n",
      "Downloading Scrapy-2.11.2-py2.py3-none-any.whl (290 kB)\n",
      "Downloading scrapy_user_agents-0.1.1-py2.py3-none-any.whl (27 kB)\n",
      "Downloading cryptography-43.0.0-cp39-abi3-manylinux_2_28_x86_64.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading itemadapter-0.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading itemloaders-1.3.1-py3-none-any.whl (12 kB)\n",
      "Downloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading parsel-1.9.1-py2.py3-none-any.whl (17 kB)\n",
      "Downloading Protego-0.3.1-py2.py3-none-any.whl (8.5 kB)\n",
      "Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
      "Downloading pyOpenSSL-24.2.1-py3-none-any.whl (58 kB)\n",
      "Downloading queuelib-1.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Downloading service_identity-24.1.0-py3-none-any.whl (12 kB)\n",
      "Downloading twisted-24.7.0-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading w3lib-2.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading zope.interface-7.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (254 kB)\n",
      "Downloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
      "Downloading user_agents-2.2.0-py3-none-any.whl (9.6 kB)\n",
      "Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
      "Downloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
      "Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "Downloading incremental-24.7.2-py3-none-any.whl (20 kB)\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
      "Downloading ua_parser-0.18.0-py2.py3-none-any.whl (38 kB)\n",
      "Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "Downloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "Installing collected packages: ua-parser, PyDispatcher, zope.interface, w3lib, user-agents, queuelib, pyasn1, protego, lxml, jmespath, itemadapter, incremental, hyperlink, cssselect, constantly, automat, Twisted, scrapy_user_agents, requests-file, pyasn1-modules, parsel, cryptography, tldextract, service-identity, pyOpenSSL, itemloaders, scrapy\n",
      "Successfully installed PyDispatcher-2.0.7 Twisted-24.7.0 automat-22.10.0 constantly-23.10.4 cryptography-43.0.0 cssselect-1.2.0 hyperlink-21.0.0 incremental-24.7.2 itemadapter-0.9.0 itemloaders-1.3.1 jmespath-1.0.1 lxml-5.3.0 parsel-1.9.1 protego-0.3.1 pyOpenSSL-24.2.1 pyasn1-0.6.0 pyasn1-modules-0.4.0 queuelib-1.7.0 requests-file-2.1.0 scrapy-2.11.2 scrapy_user_agents-0.1.1 service-identity-24.1.0 tldextract-5.1.2 ua-parser-0.18.0 user-agents-2.2.0 w3lib-2.2.1 zope.interface-7.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy scrapy_user_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'starred_repos.json': No such file or directory\n",
      "2024-08-11 07:30:11 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: github_scraper)\n",
      "2024-08-11 07:30:11 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.7.0, Python 3.10.13 (main, Jul 11 2024, 16:23:02) [GCC 9.4.0], pyOpenSSL 24.2.1 (OpenSSL 3.3.1 4 Jun 2024), cryptography 43.0.0, Platform Linux-6.5.0-1022-azure-x86_64-with-glibc2.31\n",
      "2024-08-11 07:30:11 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-08-11 07:30:11 [py.warnings] WARNING: /usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/utils/request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2024-08-11 07:30:11 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2024-08-11 07:30:11 [scrapy.extensions.telnet] INFO: Telnet Password: 9ebeba73edaa6296\n",
      "2024-08-11 07:30:12 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-08-11 07:30:12 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'github_scraper',\n",
      " 'DOWNLOAD_DELAY': 0.5,\n",
      " 'NEWSPIDER_MODULE': 'github_scraper.spiders',\n",
      " 'SPIDER_MODULES': ['github_scraper.spiders'],\n",
      " 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
      "               '(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
      "2024-08-11 07:30:12 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-08-11 07:30:12 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-08-11 07:30:12 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-08-11 07:30:12 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-08-11 07:30:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-08-11 07:30:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2024-08-11 07:30:12 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): raw.githubusercontent.com:443\n",
      "2024-08-11 07:30:12 [urllib3.connectionpool] DEBUG: https://raw.githubusercontent.com:443 \"GET /Illia-the-coder/github-scarp-bot/main/github_scraper/github_links.txt HTTP/1.1\" 200 6782\n",
      "2024-08-11 07:30:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/AaronCWacker> (referer: None)\n",
      "2024-08-11 07:30:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://github.com/AaronCWacker> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/utils/defer.py\", line 279, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/utils/python.py\", line 350, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/utils/python.py\", line 350, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/spidermiddlewares/referer.py\", line 352, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/spidermiddlewares/urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/spidermiddlewares/depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/project-1.0-py3.10.egg/github_scraper/spiders/github_scraper.py\", line 31, in parse\n",
      "    'e-mail': [x for x in response.xpath('//a/@href').getall(default='') if x.startswith('mailto:')] or '',\n",
      "TypeError: SelectorList.getall() got an unexpected keyword argument 'default'\n",
      "2024-08-11 07:30:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/Nymbo> (referer: None)\n",
      "2024-08-11 07:30:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://github.com/Nymbo> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/utils/defer.py\", line 279, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/utils/python.py\", line 350, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/utils/python.py\", line 350, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/spidermiddlewares/referer.py\", line 352, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/spidermiddlewares/urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/spidermiddlewares/depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/project-1.0-py3.10.egg/github_scraper/spiders/github_scraper.py\", line 31, in parse\n",
      "    'e-mail': [x for x in response.xpath('//a/@href').getall(default='') if x.startswith('mailto:')] or '',\n",
      "TypeError: SelectorList.getall() got an unexpected keyword argument 'default'\n",
      "2024-08-11 07:30:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/abidlabs> (referer: None)\n",
      "2024-08-11 07:30:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://github.com/abidlabs> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/utils/defer.py\", line 279, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/utils/python.py\", line 350, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/utils/python.py\", line 350, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/spidermiddlewares/referer.py\", line 352, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/spidermiddlewares/urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/spidermiddlewares/depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/project-1.0-py3.10.egg/github_scraper/spiders/github_scraper.py\", line 31, in parse\n",
      "    'e-mail': [x for x in response.xpath('//a/@href').getall(default='') if x.startswith('mailto:')] or '',\n",
      "TypeError: SelectorList.getall() got an unexpected keyword argument 'default'\n",
      "2024-08-11 07:30:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/osanseviero> (referer: None)\n",
      "2024-08-11 07:30:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://github.com/osanseviero> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/utils/defer.py\", line 279, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/utils/python.py\", line 350, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/utils/python.py\", line 350, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/spidermiddlewares/referer.py\", line 352, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/spidermiddlewares/urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/spidermiddlewares/depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/project-1.0-py3.10.egg/github_scraper/spiders/github_scraper.py\", line 31, in parse\n",
      "    'e-mail': [x for x in response.xpath('//a/@href').getall(default='') if x.startswith('mailto:')] or '',\n",
      "TypeError: SelectorList.getall() got an unexpected keyword argument 'default'\n",
      "2024-08-11 07:30:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/fffiloni> (referer: None)\n",
      "2024-08-11 07:30:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://github.com/fffiloni> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/utils/defer.py\", line 279, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/utils/python.py\", line 350, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/utils/python.py\", line 350, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/spidermiddlewares/referer.py\", line 352, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/spidermiddlewares/urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/spidermiddlewares/depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/project-1.0-py3.10.egg/github_scraper/spiders/github_scraper.py\", line 31, in parse\n",
      "    'e-mail': [x for x in response.xpath('//a/@href').getall(default='') if x.startswith('mailto:')] or '',\n",
      "TypeError: SelectorList.getall() got an unexpected keyword argument 'default'\n",
      "^C\n",
      "2024-08-11 07:30:15 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force \n",
      "2024-08-11 07:30:15 [scrapy.core.engine] INFO: Closing spider (shutdown)\n",
      "2024-08-11 07:30:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://github.com/Tonic-AI> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/utils/defer.py\", line 279, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/utils/python.py\", line 350, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/utils/python.py\", line 350, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/spidermiddlewares/referer.py\", line 352, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/spidermiddlewares/urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/spidermiddlewares/depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/project-1.0-py3.10.egg/github_scraper/spiders/github_scraper.py\", line 31, in parse\n",
      "    'e-mail': [x for x in response.xpath('//a/@href').getall(default='') if x.startswith('mailto:')] or '',\n",
      "TypeError: SelectorList.getall() got an unexpected keyword argument 'default'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File starred_repos.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscrapy crawl github_scraper -o starred_repos.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstarred_repos.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m df\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/json/_json.py:791\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    789\u001b[0m     convert_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m json_reader \u001b[38;5;241m=\u001b[39m \u001b[43mJsonReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/json/_json.py:904\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m filepath_or_buffer\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 904\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data_from_filepath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/json/_json.py:960\u001b[0m, in \u001b[0;36mJsonReader._get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    952\u001b[0m     filepath_or_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m filepath_or_buffer\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[1;32m    959\u001b[0m ):\n\u001b[0;32m--> 960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_or_buffer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    962\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal json to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    967\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    968\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File starred_repos.json does not exist"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "!rm starred_repos.json\n",
    "!scrapy crawl github_scraper -o starred_repos.json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('starred_repos.json')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'',\n",
       " 'Cisco.com',\n",
       " 'anuragks.tech',\n",
       " 'apolinar.io',\n",
       " 'applio.org',\n",
       " 'buffalo.edu',\n",
       " 'diwank.name',\n",
       " 'dlsu.edu.ph',\n",
       " 'endrcompany.com',\n",
       " 'fltr.pw',\n",
       " 'gmail.com',\n",
       " 'gmal.com',\n",
       " 'hotmail.com',\n",
       " 'icloud.com',\n",
       " 'indonesiancryptolaw.com',\n",
       " 'inpocket.ai',\n",
       " 'live.co.za',\n",
       " 'mail2.sysu.edu.cn',\n",
       " 'metafunctor.com',\n",
       " 'mindwavestudios.com',\n",
       " 'mt-oneblock.net',\n",
       " 'outlook.com',\n",
       " 'programmer.net',\n",
       " 'sajalsharma.com',\n",
       " 'sea.com',\n",
       " 'students.iitmandi.ac.in',\n",
       " 'tju.edu.cn',\n",
       " 'tonic-ai.com',\n",
       " 'ucsd.edu',\n",
       " 'vinuni.edu.vn'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([x.split('@')[-1] for x in df['email'].unique()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
